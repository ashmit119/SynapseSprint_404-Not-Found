# -*- coding: utf-8 -*-
"""recommender.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_MAPWr2nW64K0Pkc69-f-fCFTgkrQTbm
"""

#SECTION 1:IMPORT LIBRARIES
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler

#SECTION 2:PROCESS ALL THE CSV FILES
apps_info = pd.read_csv('apps_info.csv')
games_info = pd.read_csv('games_info.csv')
apps_reviews = pd.read_csv('apps_reviews.csv')
games_reviews = pd.read_csv('games_reviews.csv')
items = pd.DataFrame()
#SECTION 3:PREPARE DATA ACCORDINGLY
# Drop columns not needed for recommendation
apps_info = apps_info.drop(columns=['score', 'ratings_count', 'downloads', 'content_rating', 'section'])
games_info = games_info.drop(columns=['score', 'ratings_count', 'downloads', 'content_rating', 'section'])

apps_reviews = apps_reviews.drop(columns=['review_date', 'helpful_count'])
games_reviews = games_reviews.drop(columns=['review_date', 'helpful_count'])

# Rename for consistency
apps_info.rename(columns={'app_id': 'id', 'app_name': 'name'}, inplace=True)
games_info.rename(columns={'game_id': 'id', 'game_name': 'name'}, inplace=True)
apps_reviews.rename(columns={'app_id': 'id'}, inplace=True)
games_reviews.rename(columns={'game_id': 'id'}, inplace=True)

#SECTION 4:SENTIMENT ANALYSIS
reviews = pd.concat([apps_reviews, games_reviews], ignore_index=True)

def sentiment_label(score):
    if score >= 4:
        return 'positive'
    elif score == 3:
        return 'neutral'
    else:
        return 'negative'

reviews['sentiment'] = reviews['review_score'].apply(sentiment_label)

reviews_agg = reviews.groupby('id').agg({
    'review_text': lambda x: " ".join(x),
    'sentiment': lambda x: " ".join(x)
}).reset_index()

#SECTION 5:MERGING DATA
apps_info['type'] = 'app'
games_info['type'] = 'game'

items_info = pd.concat([apps_info, games_info], ignore_index=True)
items = items_info.merge(reviews_agg, on='id', how='left')
items['review_text'] = items['review_text'].fillna('')
items['sentiment'] = items['sentiment'].fillna('')

items['tags'] = (items['description'].fillna('') + " " +
                 items['categories'].fillna('') + " " +
                 items['review_text'] + " " +
                 items['sentiment']).str.lower()

#SECTION 6:BUILDING THE BASE OF THE RECOMMENDATION SYSTEM
#Build content-based similarity matrix (filtered by type and category)
def get_content_similarity(subset):
    cv = CountVectorizer(max_features=5000, stop_words='english')
    vectors = cv.fit_transform(subset['tags']).toarray()
    return cosine_similarity(vectors)
#Build collaborative similarity matrix based on review text TF-IDF
def get_collaborative_similarity(subset):
    tfidf = TfidfVectorizer(max_features=5000, stop_words='english')
    review_vectors = tfidf.fit_transform(subset['review_text'])
    return cosine_similarity(review_vectors)

#SECTION 7:BUILDING THE HYBRID RECOMMENDATION SYSTEM(COMBINING THE COLLABORATIVE FILTERING AND CONTENT BASED METHODS)
def hybrid_recommend(item_name,items, top_n=5):
    if item_name not in items['name'].values:
        print(f"Item '{item_name}' not found.")
        return

    # Get item info
    item = items[items['name'] == item_name].iloc[0]
    item_type = item['type']
    item_category = item['categories']

    # Filter items of same type and overlapping category
    def category_match(cat_str):
        item_cats = set([c.strip().lower() for c in item_category.split(',')])
        other_cats = set([c.strip().lower() for c in cat_str.split(',')])
        return len(item_cats.intersection(other_cats)) > 0

    filtered_items = items[(items['type'] == item_type) & (items['categories'].apply(category_match))].reset_index(drop=True)

    # Compute content similarity matrix for filtered items
    content_sim = get_content_similarity(filtered_items)

    # Compute collaborative similarity matrix for filtered items
    collab_sim = get_collaborative_similarity(filtered_items)

    # Normalize similarity matrices
    scaler = MinMaxScaler()
    content_sim_norm = scaler.fit_transform(content_sim)
    collab_sim_norm = scaler.fit_transform(collab_sim)

    # Find index of the item in filtered_items
    try:
        idx = filtered_items[filtered_items['name'] == item_name].index[0]
    except IndexError:
        print("Item not found in filtered subset.")
        return

    # Extract similarity vectors for the item (1D arrays)
    content_sim_vector = content_sim_norm[idx]
    collab_sim_vector = collab_sim_norm[idx]

    # Hybrid similarity vector by summing content and collaborative similarities
    hybrid_sim = content_sim_vector + collab_sim_vector

    # Get indices of top N recommendations excluding the item itself
    recommended_indices = hybrid_sim.argsort()[::-1]  # descending order
    recommended_indices = [i for i in recommended_indices if i != idx][:top_n]

    print(f"Top {top_n} hybrid recommendations similar to '{item_name}':")
    for i in recommended_indices:
        print(filtered_items.iloc[i]['name'])

#SECTION 8:THE FINAL OUTPUT(WHERE YOU INPUT THE NAME OF THE APPLICATION AND GET 5 TOP RECOMMENDATIONS SIMILAR TO YOUR INPUT)
hybrid_recommend('MONOPOLY',items, top_n=5)